{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, LSTM, TimeDistributed\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directories for training and testing data\n",
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/validation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dataframe for images and their labels\n",
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir, label)):\n",
    "            image_paths.append(os.path.join(dir, label, imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n",
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataframe for training and testing data\n",
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract features from images\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, color_mode='grayscale')\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features), 48, 48, 1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28821/28821 [02:19<00:00, 207.09it/s]\n",
      "100%|██████████| 7066/7066 [00:37<00:00, 189.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract features for training and testing datasets\n",
    "train_features = extract_features(train['image'])\n",
    "test_features = extract_features(test['image'])\n",
    "\n",
    "# Normalize the data by dividing by 255\n",
    "x_train = train_features / 255.0\n",
    "x_test = test_features / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label encoding and one-hot encoding the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(train['label'])\n",
    "\n",
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(features, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(features) - sequence_length + 1):\n",
    "        sequences.append(features[i:i + sequence_length])\n",
    "    return np.array(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to apply augmentation to each frame and then create sequences\n",
    "def augment_and_create_sequences(features, labels, sequence_length, datagen, batch_size):\n",
    "    augmented_features = []\n",
    "    augmented_labels = []\n",
    "    num_samples = len(features)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # Extract a batch\n",
    "        batch_x = features[i:i + batch_size]\n",
    "        batch_y = labels[i:i + batch_size]\n",
    "        \n",
    "        # Apply augmentation to each image in the batch\n",
    "        for j in range(len(batch_x)):\n",
    "            # Expand the dimensions to (1, 48, 48, 1) for augmentation\n",
    "            image = batch_x[j].reshape(1, 48, 48, 1)\n",
    "            aug_iter = datagen.flow(image, batch_size=1)\n",
    "            aug_image = next(aug_iter)[0]  # Get the augmented image\n",
    "            \n",
    "            augmented_features.append(aug_image)\n",
    "            augmented_labels.append(batch_y[j])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    augmented_features = np.array(augmented_features)\n",
    "    augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "    # Now create sequences from the augmented images\n",
    "    x_seq = create_sequences(augmented_features, sequence_length)\n",
    "    y_seq = augmented_labels[:len(x_seq)]\n",
    "    \n",
    "    return x_seq, y_seq\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "sequence_length = 5  # Adjust sequence length as per your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply augmentation and create sequences for training data\n",
    "x_train_seq, y_train_seq = augment_and_create_sequences(x_train, y_train, sequence_length, datagen, batch_size=128)\n",
    "\n",
    "# No data augmentation for the test set\n",
    "x_test_seq = create_sequences(x_test, sequence_length)\n",
    "y_test_seq = y_test[:len(x_test_seq)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\Important Documents\\Manan\\2 UK\\MSc\\Dissertation\\archive\\env\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# CNN-LSTM model definition\n",
    "model = Sequential()\n",
    "\n",
    "# CNN layers (for spatial feature extraction)\n",
    "model.add(TimeDistributed(Conv2D(128, kernel_size=(3, 3), activation='relu'), input_shape=(sequence_length, 48, 48, 1)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.4)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(256, kernel_size=(3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.4)))\n",
    "\n",
    "# Flatten the output of CNN\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# LSTM layer for temporal pattern recognition\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2213s\u001b[0m 10s/step - accuracy: 0.2364 - loss: 1.8320 - val_accuracy: 0.3352 - val_loss: 1.6148\n",
      "Epoch 2/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1396s\u001b[0m 6s/step - accuracy: 0.3310 - loss: 1.6511 - val_accuracy: 0.3676 - val_loss: 1.5118\n",
      "Epoch 3/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1264s\u001b[0m 6s/step - accuracy: 0.4054 - loss: 1.5102 - val_accuracy: 0.4919 - val_loss: 1.2712\n",
      "Epoch 4/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1261s\u001b[0m 6s/step - accuracy: 0.5229 - loss: 1.2799 - val_accuracy: 0.3973 - val_loss: 1.9010\n",
      "Epoch 5/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1234s\u001b[0m 5s/step - accuracy: 0.6929 - loss: 0.8609 - val_accuracy: 0.5347 - val_loss: 1.6833\n",
      "Epoch 6/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1219s\u001b[0m 5s/step - accuracy: 0.8833 - loss: 0.3455 - val_accuracy: 0.4884 - val_loss: 2.4511\n",
      "Epoch 7/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1223s\u001b[0m 5s/step - accuracy: 0.9555 - loss: 0.1404 - val_accuracy: 0.5181 - val_loss: 2.5193\n",
      "Epoch 8/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1219s\u001b[0m 5s/step - accuracy: 0.9736 - loss: 0.0802 - val_accuracy: 0.5038 - val_loss: 2.6310\n",
      "Epoch 9/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1220s\u001b[0m 5s/step - accuracy: 0.9834 - loss: 0.0525 - val_accuracy: 0.4887 - val_loss: 2.7733\n",
      "Epoch 10/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1227s\u001b[0m 5s/step - accuracy: 0.9870 - loss: 0.0391 - val_accuracy: 0.5317 - val_loss: 2.5874\n",
      "Epoch 11/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1220s\u001b[0m 5s/step - accuracy: 0.9885 - loss: 0.0366 - val_accuracy: 0.5217 - val_loss: 2.7781\n",
      "Epoch 12/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1240s\u001b[0m 5s/step - accuracy: 0.9903 - loss: 0.0302 - val_accuracy: 0.5355 - val_loss: 2.8684\n",
      "Epoch 13/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1237s\u001b[0m 5s/step - accuracy: 0.9922 - loss: 0.0229 - val_accuracy: 0.5160 - val_loss: 3.0513\n",
      "Epoch 14/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1233s\u001b[0m 5s/step - accuracy: 0.9921 - loss: 0.0248 - val_accuracy: 0.5262 - val_loss: 2.9460\n",
      "Epoch 15/15\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1242s\u001b[0m 5s/step - accuracy: 0.9920 - loss: 0.0276 - val_accuracy: 0.5251 - val_loss: 3.1764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x267b65e7ce0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with augmented sequence data\n",
    "model.fit(x_train_seq, y_train_seq, epochs=15, batch_size=128, validation_data=(x_test_seq, y_test_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector_lstm.json\", 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector_lstm.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the saved model and weights\n",
    "json_file = open(\"emotiondetector_lstm.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"emotiondetector_lstm.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction function\n",
    "label = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "def ef(image):\n",
    "    img = load_img(image, color_mode='grayscale')\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1, 48, 48, 1)\n",
    "    return feature / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of sad\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node sequential_1/time_distributed_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_1/time_distributed_1/strided_slice, sequential_1/time_distributed_1/convolution/ReadVariableOp)' with input shapes: [1,48,1,1], [3,3,1,128].\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(1, 48, 48, 1, 1), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal image is of sad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[1;32m----> 5\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n",
      "File \u001b[1;32mw:\\Important Documents\\Manan\\2 UK\\MSc\\Dissertation\\archive\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mw:\\Important Documents\\Manan\\2 UK\\MSc\\Dissertation\\archive\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node sequential_1/time_distributed_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_1/time_distributed_1/strided_slice, sequential_1/time_distributed_1/convolution/ReadVariableOp)' with input shapes: [1,48,1,1], [3,3,1,128].\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(1, 48, 48, 1, 1), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "image = 'images/train/sad/42.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is\", pred_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
